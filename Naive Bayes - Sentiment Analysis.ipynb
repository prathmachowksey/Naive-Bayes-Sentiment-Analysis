{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Fw8iHEvEp-wb",
    "outputId": "523862b7-58aa-422d-ed27-82d70a623fec"
   },
   "outputs": [],
   "source": [
    "#Open the dataset 'a1_d3.txt', it is stored in the same directory.\n",
    "import io\n",
    "data_path = './a1_d3.txt'\n",
    "with open(data_path, 'r') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "\n",
    "#'lines' stores the instances in a raw format.\n",
    "# print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jhxnUacFqEIE",
    "outputId": "19900e65-3ff3-4b7e-b779-b826b27c52ca"
   },
   "outputs": [],
   "source": [
    "#######Preprocessing\n",
    "#Some basic preprocessing steps to work with a better version of the data.\n",
    "\n",
    "from string import punctuation \n",
    "\n",
    "#Create a punctuation list, which would help in identifying the punctuations\n",
    "punct =[]\n",
    "punct += list(punctuation)\n",
    "punct += 'â€™'\n",
    "punct.remove(\"'\")\n",
    "\n",
    "#This function removes the punctuations from the input text and returns the processed text.\n",
    "def remove_punctuations(text):\n",
    "    for punctuation in punct:\n",
    "        text = text.replace(punctuation, ' ')\n",
    "    return text\n",
    "\n",
    "#These are the common leftovers, after removing stopwords or punctuations\n",
    "leftovers = [\"'ve\",\"'s\",\"'m\",\"i'm\",\"n't\"]\n",
    "\n",
    "#These are the common stopwords for English Language, which are needed to be removed from the Vocabulary corpus.\n",
    "stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\",\n",
    "             \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\",\n",
    "             \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \n",
    "             \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\",\n",
    "             \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\",\n",
    "             \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \n",
    "             \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\",\n",
    "             \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\",\n",
    "             \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "\n",
    "_stopwords = set(stopwords+leftovers)\n",
    "\n",
    "def Process(lines):\n",
    "    processedLines=[]\n",
    "    for line in lines:\n",
    "      processedLines.append(ProcessUtil(line))\n",
    "    return processedLines\n",
    "\n",
    "#This utility function does the following steps:\n",
    "########### 1. converts text to lower case\n",
    "########### 2. removes punctuations\n",
    "########### 3. tokenises the sentences into words.\n",
    "def ProcessUtil(line):\n",
    "\n",
    "    line = line.lower() # convert text to lower-case\n",
    "    line = remove_punctuations(line) #remove punctuations\n",
    "    line = line.split() # tokenise the line into words by splitting whitespaces.\n",
    "    return [word for word in line if word not in _stopwords]\n",
    "\n",
    "processedLines = Process(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "_DKMo-5ZqGsL",
    "outputId": "1252b04e-d1c7-43e4-a7d7-70fb6c92c026"
   },
   "outputs": [],
   "source": [
    "processedData=[]\n",
    "# [['a',''b'.....'label(0/1)'],[]]\n",
    "for l in processedLines:\n",
    "  if(len(l)):\n",
    "    words = []\n",
    "    label = -1\n",
    "    for x in range(len(l)-1):\n",
    "      words.append(l[x])\n",
    "    label = l[len(l)-1]\n",
    "    entry=[]\n",
    "    entry.append(words)\n",
    "    entry.append(label)\n",
    "    processedData.append(entry)\n",
    "\n",
    "# print(processedData)\n",
    "\n",
    "#processedData is a list of listZs'\n",
    "#each listZ is a list containing 2 elements, 0th element is the list of words(tokens), 1st element is the associated label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_wTEl3oOqQQE"
   },
   "outputs": [],
   "source": [
    "#This function creates a word-frequency dictionary for both the classes\n",
    "def CreateWordcountDicts(training_instances):\n",
    "    class0Wordcount={}\n",
    "    class1Wordcount={}\n",
    "\n",
    "    \n",
    "    for i in training_instances:\n",
    "      label = i[1]\n",
    "    \n",
    "      #if label is 1, add to class1 dictionary\n",
    "      if(label=='1'):\n",
    "        words = i[0]\n",
    "        for w in words:\n",
    "          if w not in class1Wordcount.keys():\n",
    "            class1Wordcount[w]=1\n",
    "          else:\n",
    "            class1Wordcount[w]+=1\n",
    "\n",
    "      #if label is 0, add to class0 dictionary\n",
    "      if(label=='0'):\n",
    "        words = i[0]\n",
    "        for w in words:\n",
    "          if w not in class0Wordcount.keys():\n",
    "            class0Wordcount[w]=1\n",
    "          else:\n",
    "            class0Wordcount[w]+=1\n",
    "\n",
    "    return class0Wordcount,class1Wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sG3RVjmhq6PL"
   },
   "outputs": [],
   "source": [
    "#This function creates a word-frequency dictionary for the training instances\n",
    "def GetListofDistinctWords(testing_instances):\n",
    "    #This would be a list of dicts.\n",
    "    wordsInTestData=[]\n",
    "    \n",
    "    for i in testing_instances:\n",
    "      words=i[0]\n",
    "      toAdd={}\n",
    "      for w in words:\n",
    "        if w not in toAdd.keys():\n",
    "          toAdd[w]=1\n",
    "        else:\n",
    "          toAdd[w]+=1\n",
    "      \n",
    "      wordsInTestData.append(toAdd)\n",
    "        \n",
    "    return wordsInTestData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FjasuP4VrVyz"
   },
   "outputs": [],
   "source": [
    "#This function just extracts the labels associated with the training instances.\n",
    "def GetActualLabels(testing_instances):\n",
    "    actual = []\n",
    "    for i in testing_instances:\n",
    "        #Store the actual class label of testing instance in actual list.\n",
    "        label = i[1]\n",
    "        actual.append(label)\n",
    "        \n",
    "    return actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZokwlJLrqnU"
   },
   "outputs": [],
   "source": [
    "#This function calculates the prior probablities for both classes, using the training instances.\n",
    "def CalcPriors(training_instances):\n",
    "\n",
    "    prior1=0\n",
    "    prior0=0\n",
    "    numClass1=0\n",
    "    numClass0=0\n",
    "\n",
    "    for i in training_instances:\n",
    "      label=i[1]\n",
    "      if(label=='1'):\n",
    "        numClass1+=1\n",
    "      if(label=='0'):\n",
    "        numClass0+=1\n",
    "\n",
    "    total = numClass1 + numClass0\n",
    "\n",
    "    prior1 = numClass1/total\n",
    "    prior0 = numClass0/total\n",
    "\n",
    "#     print(prior1)\n",
    "#     print(prior0)\n",
    "\n",
    "    return prior1, prior0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vVYulAxisFhi"
   },
   "outputs": [],
   "source": [
    "#This is the function that implements the Naive Bayes Classifier\n",
    "#The major steps involved are shown in the function definition below.\n",
    "def Classify(class1Wordcount,class0Wordcount,wordsInTestData,prior1,prior0):\n",
    "\n",
    "    predictions = []\n",
    "    V = len(class1Wordcount.keys())+len(class0Wordcount.keys())\n",
    "    alpha = 0.001\n",
    "\n",
    "    for i in wordsInTestData:\n",
    "        \n",
    "        words = i.keys()\n",
    "        ###For Class 1\n",
    "        #Step1 - Finding Conditional Probabilities \n",
    "\n",
    "        WordCntClass1=sum(class1Wordcount.values())\n",
    "\n",
    "        #calculating P(Xi/y=1) = freq(Xi)/(Word count of all words in class 1)\n",
    "        \n",
    "        #Likelihood function P(X/y=1) = product of all P(Xi/y=1)\n",
    "        likelihood1=1\n",
    "        \n",
    "        for word in words:\n",
    "          if word in class1Wordcount.keys():\n",
    "            likelihood1*=(alpha+class1Wordcount[word])/(V+WordCntClass1)\n",
    "          #Check else condition daalna hai ya nahi...  \n",
    "          else:\n",
    "            likelihood1*=(alpha)/(V+WordCntClass1)\n",
    "            \n",
    "            likelihood1 = (likelihood1)**i[word]\n",
    "        #Step2 - Finding posterior probability, p1 = P(y=1/X) , not dividing by the product of evidence probabilities, i.em P(Xi)'s\n",
    "\n",
    "        p1 = likelihood1*prior1\n",
    "\n",
    "        ###For Class 0\n",
    "        #Step1 - Finding Conditional Probabilities \n",
    "\n",
    "        WordCntClass0=sum(class0Wordcount.values())\n",
    "\n",
    "\n",
    "        #calculating P(Xi/y=0) = freq(Xi)/(Word count of all words in class 1)\n",
    "        #likelihood function P(X/y=0) = product of all P(Xi/y=0)\n",
    "        likelihood0=1\n",
    "        \n",
    "        for word in words:\n",
    "          if word in class0Wordcount.keys():\n",
    "            likelihood0*=(alpha+class0Wordcount[word])/(V+WordCntClass0)\n",
    "          #Check else condition daalna hai ya nahi...  \n",
    "          else:\n",
    "            likelihood0*=(alpha)/(V+WordCntClass0)\n",
    "            \n",
    "            likelihood0 = (likelihood0)**i[word]\n",
    "        #Step2 - Finding posterior probability, p0 = P(y=0/X) , not dividing by the product of evidence probabilities, i.em P(Xi)'s\n",
    "\n",
    "        p0 = likelihood0*prior0\n",
    "\n",
    "        #Now based on values of p1 and p0 , predict the class of the training instance.\n",
    "\n",
    "        if(p1>=p0):\n",
    "          predictions.append('1')\n",
    "        else:\n",
    "          predictions.append('0')\n",
    "\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v-8wOEV8sof5"
   },
   "outputs": [],
   "source": [
    "#This is to calculate the Accuracy, Precision, Recall and F-Score for the resultts of the classifier.\n",
    "def getAccuracyAndFScore(predictions,actual):\n",
    "    TP, TN, FP, FN = 0 , 0 , 0 , 0\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i]== '1' and actual[i]== '1' :\n",
    "          TP += 1\n",
    "        elif predictions[i]== '0' and actual[i]== '0' :\n",
    "          TN += 1\n",
    "        elif predictions[i]== '1' and actual[i]== '0' :\n",
    "          FP += 1\n",
    "        elif predictions[i]== '0' and actual[i]== '1' :\n",
    "          FN += 1\n",
    "    print(TP,TN,FP,FN)    \n",
    "    precision = TP/(TP+FP)\n",
    "    recall = TP/(TP+FN)\n",
    "    fscore = 2*precision*recall/ (precision + recall)\n",
    "    accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "    \n",
    "    return accuracy,fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "lmsrZNTbqIyj",
    "outputId": "b33453c3-87ca-4bc1-b4e1-198f4fc9f8ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 74 34 22\n",
      "73 76 19 32\n",
      "88 66 28 18\n",
      "76 78 21 25\n",
      "72 79 25 24\n"
     ]
    }
   ],
   "source": [
    "#5 Fold Cross Validation\n",
    "from sklearn.model_selection import KFold\n",
    "from numpy import array\n",
    "data = processedData\n",
    "kfold = KFold(5, True, 1)\n",
    "# enumerate splits\n",
    "fold_accuracies = []\n",
    "fold_fscores = []\n",
    "for train, test in kfold.split(data):\n",
    "  training_data=[]\n",
    "  testing_data=[]\n",
    "  for tr in range(len(train)):\n",
    "    training_data.append(data[train[tr]])\n",
    "  for te in range(len(test)):\n",
    "    testing_data.append(data[test[te]])\n",
    "    #for each fold, classify the testing instances using the model trained by the training instances.\n",
    "\n",
    "  training_instances=training_data\n",
    "  testing_instances=testing_data\n",
    "  \n",
    "  class0Wordcount,class1Wordcount = CreateWordcountDicts(training_instances)\n",
    "\n",
    "  #Create list of distinct words and store their frequencies in the testing instances\n",
    "  wordsInTestData = GetListofDistinctWords(testing_instances)\n",
    "  #Now, wordsInTestData would be a list of dicts, where each dict would contain the unique word-frequency tuples present in the testing instance.\n",
    "\n",
    "  #Calculate prior probabilities.\n",
    "  prior1, prior0 = CalcPriors(training_instances)\n",
    "\n",
    "  #Now predict the class of training instances, using Naive Bayes Classifier\n",
    "  predictions = Classify(class1Wordcount,class0Wordcount,wordsInTestData,prior1,prior0)\n",
    "\n",
    "  #Store the labels of the testing instances\n",
    "  actual = GetActualLabels(testing_instances)\n",
    "  \n",
    "  #calculating accuracy of model\n",
    "  accuracy,fscore = getAccuracyAndFScore(predictions,actual)\n",
    "  fold_accuracies.append(accuracy)\n",
    "  fold_fscores.append(fscore)\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ez56oDjdqNwV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72, 0.745, 0.77, 0.77, 0.755]\n",
      "[0.7142857142857143, 0.7411167512690355, 0.7927927927927928, 0.7676767676767676, 0.7461139896373057]\n",
      "\n",
      "Accuracy of classifier:   0.752 Â± 0.02\n",
      "\n",
      "F-score of classifier:   0.7523972031323232 Â± 0.03\n"
     ]
    }
   ],
   "source": [
    "#This cell just reports the accuracies and Fscores for each fold.\n",
    "\n",
    "import statistics\n",
    "print(fold_accuracies)\n",
    "print(fold_fscores)\n",
    "acc_mean = statistics.mean(fold_accuracies)\n",
    "acc_std = statistics.pstdev(fold_accuracies)\n",
    "acc_std = \"{:.2f}\".format(acc_std)\n",
    "\n",
    "fs_mean = statistics.mean(fold_fscores)\n",
    "fs_std = statistics.pstdev(fold_fscores)\n",
    "fs_std = \"{:.2f}\".format(fs_std)\n",
    "\n",
    "print('\\nAccuracy of classifier:  ',acc_mean,'Â±',acc_std)\n",
    "print('\\nF-score of classifier:  ',fs_mean,'Â±',fs_std)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NaiveBayesWith5Fold .ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
